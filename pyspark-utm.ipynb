{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcp6       0      0 :::4040                 :::*                    LISTEN      27266/java          \r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"etl\").getOrCreate()\n",
    "#spark.sparkContext.getConf().getAll()\n",
    "!netstat -anp |grep 4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outsider of spark transformation \n",
    "# Convert the input CSV to Parquet using a separate script prior loading in spark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#!pip install pyarrow\n",
    "\n",
    "def write_parquet_file():\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df.to_parquet('data.parquet',compression='snappy')\n",
    "\n",
    "write_parquet_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+-------+---------+-----------------+\n",
      "|   anonymous_user_id|                 url|      time|browser|       os|screen_resolution|\n",
      "+--------------------+--------------------+----------+-------+---------+-----------------+\n",
      "|b527ad6c-1e79-4ae...|https://preview.h...|1523551855| Chrome|    Linux|          800x600|\n",
      "|2847753f-df48-436...|https://www.nasa....|1523549796| Chrome|Chrome OS|         1366x768|\n",
      "|7540e6a6-bd75-4f2...|https://www.nasa....|1523549868| Chrome| Mac OS X|        2560x1440|\n",
      "+--------------------+--------------------+----------+-------+---------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, anonymous_user_id: string, url: string, time: string, browser: string, os: string, screen_resolution: string]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data to temporary view \n",
    "parquetFile = spark.read.parquet(\"data.parquet\")\n",
    "parquetFile.createOrReplaceTempView(\"source_df\")\n",
    "\n",
    "sql  = \"select  * from source_df\"\n",
    "full_data = spark.sql(sql)\n",
    "full_data.show(3)\n",
    "full_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+-----+\n",
      "|  utm_medium|    utm_source|count|\n",
      "+------------+--------------+-----+\n",
      "|         cpc|        google|52386|\n",
      "|        null|          null|31931|\n",
      "|        null|      homepage| 3897|\n",
      "|       email|      hs_email| 1001|\n",
      "|  paidsocial|      linkedin|  363|\n",
      "|         cpc|          null|  161|\n",
      "|  paidsocial|      facebook|   93|\n",
      "|       email| hs_automation|   62|\n",
      "|      social|       twitter|   18|\n",
      "|paid_listing|      capterra|   17|\n",
      "|      social|   twitter.com|    6|\n",
      "|Display%20Ad|  Office%20Kit|    4|\n",
      "|         cpc|           ppc|    2|\n",
      "|        null|homepage%C2%A0|    2|\n",
      "|    Facebook|        Social|    1|\n",
      "|     adwords|          null|    1|\n",
      "|         cpc|        carbon|    1|\n",
      "|         cpc|          bing|    1|\n",
      "|    referral|        wework|    1|\n",
      "|       email|      sendgrid|    1|\n",
      "+------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql1 = \"\"\"\n",
    "select \n",
    "split(split(url,'utm_medium=')[1] , '&' )[0] as utm_medium ,\n",
    "split(split(url,'utm_source=')[1] , '&' )[0] as utm_source , \n",
    "url  \n",
    "from source_df\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#sql = \"\"\"\n",
    "#select split(url,'utm_medium=')[1]  , url  as utm_medium , \n",
    "#split(split(url,'utm_campaign=')[1] , '&' )[0] as utm_campaign\n",
    "#from source_df\n",
    "\n",
    "\n",
    "\n",
    "url_data = spark.sql(sql1)\n",
    "#url_data.show(5, False)\n",
    "url_data.createOrReplaceTempView(\"url_data_view\")\n",
    "\n",
    "sql_top_medium_source = \"\"\"\n",
    "select  \n",
    "utm_medium , utm_source,\n",
    "count(url) as count \n",
    "from url_data_view\n",
    "group by utm_medium , utm_source\n",
    "order by count desc \n",
    "\"\"\"\n",
    "top_medium_source = spark.sql(sql_top_medium_source)\n",
    "top_medium_source.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+\n",
      "|   anonymous_user_id|          timestamp|      date|\n",
      "+--------------------+-------------------+----------+\n",
      "|b527ad6c-1e79-4ae...|2018-04-13 02:50:55|2018-04-13|\n",
      "|2847753f-df48-436...|2018-04-13 02:16:36|2018-04-13|\n",
      "|7540e6a6-bd75-4f2...|2018-04-13 02:17:48|2018-04-13|\n",
      "+--------------------+-------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql = \"select cast(from_unixtime(time) , 'YYYY-MM-DD HH:MM:SS' )  as date from source_df\"\n",
    "epoch2date = \"\"\"select \n",
    "anonymous_user_id,\n",
    "to_timestamp(from_unixtime(time), 'yyyy-MM-dd HH:mm:ss') as timestamp ,\n",
    "to_date(from_unixtime(time), 'yyyy-MM-dd HH:mm:ss') as date \n",
    "from source_df\"\"\"\n",
    "\n",
    "transformed_df = spark.sql(epoch2date)\n",
    "transformed_df.show(3)\n",
    "transformed_df.createOrReplaceTempView(\"transformed_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      date|distinct_users|\n",
      "+----------+--------------+\n",
      "|2018-04-01|           245|\n",
      "|2018-04-02|          2414|\n",
      "|2018-04-03|          3401|\n",
      "|2018-04-04|          3023|\n",
      "|2018-04-05|          5398|\n",
      "|2018-04-06|          4866|\n",
      "|2018-04-07|          4815|\n",
      "|2018-04-08|          3941|\n",
      "|2018-04-09|          4146|\n",
      "|2018-04-10|          6304|\n",
      "|2018-04-11|          5796|\n",
      "|2018-04-12|          5771|\n",
      "|2018-04-13|          5565|\n",
      "|2018-04-14|          4179|\n",
      "|2018-04-15|          1406|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "distinct_users_per_day = \"\"\"\n",
    "select \n",
    "date ,\n",
    "count(distinct(anonymous_user_id)) as distinct_users \n",
    "from transformed_df\n",
    "group by date\n",
    "order by date asc \n",
    "\n",
    "\"\"\"\n",
    "s = spark.sql(distinct_users_per_day)\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
