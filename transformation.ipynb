{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import logging_class\n",
    "import pandasql as ps\n",
    "from urllib.parse import urlparse , parse_qs\n",
    "import sys \n",
    "\n",
    "class Transformation:\n",
    "    \"\"\"\n",
    "    Multiple Transformations \n",
    "      1. Convert time column in to a human readable format. \n",
    "      2. Extract medium ,source and path from URL.\n",
    "      3. List Top count medium and source.\n",
    "      4. List distinct users count, min time , max time for a subset of records.\n",
    "      5. LIst distinct users count by day. \n",
    "    \n",
    "    \"\"\"\n",
    "    log_file_name = 'etl_log.out'\n",
    "    log = logging_class.setup_logging(log_file_name)\n",
    "    #pysqldf = lambda q: ps.sqldf(q, globals())\n",
    "    \n",
    "    def __init__(self , source_df ):\n",
    "        \n",
    "        self.source_df = source_df\n",
    "     \n",
    "    def set_time_readable(self):\n",
    "        \n",
    "        try:\n",
    "            self.log.logger.info('start: Change column time to human readable format- ')\n",
    "            self.source_df['time']=(pd.to_datetime(self.source_df['time'],unit='s'))\n",
    "            self.log.logger.info(' Success: Change column time to human readable format')\n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing set_time_readable \" )\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "            \n",
    "                \n",
    "    def runSQL(self , v_sql):\n",
    "        \"\"\"\n",
    "        execute sql and return dataframe using pandassql \n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_out = ps.sqldf(v_sql)\n",
    "            self.log.logger.info(\"Executed SQL successfully \" +v_sql )\n",
    "            return df_out \n",
    "        \n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing SQL query \" +v_sql)\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "            \n",
    "        \n",
    "    def get_medium(self, x):\n",
    "        \"\"\"\n",
    "        Extract Medium using urlparse to extract Medium - look for string 'utm_medium'\n",
    "        Return multiple medium as one string appended using separator '---'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            v_medium = 'utm_medium'\n",
    "            if v_medium in parse_qs(urlparse(x['url']).query).keys():\n",
    "                v_medium_list = parse_qs(urlparse(x['url']).query)['utm_medium']\n",
    "                v_medium_set = set(v_medium_list)\n",
    "                v_uniq_medium_list = list(v_medium_set)\n",
    "\n",
    "                v_ret = None\n",
    "                for i in range (len(v_medium_set)):\n",
    "                        if i == 0:\n",
    "                            v_ret = v_uniq_medium_list[0]\n",
    "                        else:\n",
    "                            v_ret = v_ret+ \"---\" +v_uniq_medium_list[i]\n",
    "                return v_ret \n",
    "            \n",
    "                #return (v_medium_list[0])\n",
    "\n",
    "            else: \n",
    "                return None \n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing get_medium function\")\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "             \n",
    "    \n",
    "    def get_source(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Extract Source using urlparse to extract Soruce - look for string 'utm_source'\n",
    "        Return multiple medium as one string appended using separator '---'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            v_source = 'utm_source'\n",
    "            if v_source in parse_qs(urlparse(x['url']).query).keys():\n",
    "                v_source_list = parse_qs(urlparse(x['url']).query)['utm_source']\n",
    "                v_source_set = set (v_source_list)\n",
    "                v_uniq_source_list = list(v_source_set)\n",
    "                v_ret = None \n",
    "                for i in range (len(v_uniq_source_list)):\n",
    "                    if i == 0:\n",
    "                        v_ret = v_uniq_source_list[0]\n",
    "                    else:\n",
    "                        v_ret = v_ret+ \"---\" +v_uniq_source_list[i]\n",
    "                return v_ret \n",
    "\n",
    "\n",
    "            else: \n",
    "                return None \n",
    "        \n",
    "        except:  \n",
    "            self.log.logger.error(\"Error executing get_source function\")\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "    def get_path(self , x):\n",
    "        \"\"\"\n",
    "        URL will always return path , unless wrong data is captured . Blank URL will have a string :'/'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if 1:\n",
    "                return urlparse(x['url']).path\n",
    "        \n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing get_source function\")\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "             \n",
    "            \n",
    "        \n",
    "    def set_medium_source_path(self):\n",
    "        \"\"\"\n",
    "        Modify soruce Data frame. Add 3 columns \n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.log.logger.info('start: Extract Medium')\n",
    "            self.source_df['utm_medium'] = self.source_df.apply(self.get_medium, axis =1)\n",
    "            self.log.logger.info('End : Extract Medium')\n",
    "            \n",
    "            self.log.logger.info('start: Extract Source')\n",
    "            self.source_df['utm_source'] = self.source_df.apply(self.get_source, axis =1)\n",
    "            self.log.logger.info('End : Extract Source')\n",
    "            \n",
    "            self.log.logger.info('start: Extract Path')\n",
    "            self.source_df['path'] = self.source_df.apply(self.get_path, axis=1)\n",
    "            self.log.logger.info('End : Extract Path')\n",
    "            \n",
    "                   \n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing set_medium_source function\")\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "      \n",
    "        \n",
    "    def final_dataframe(self):\n",
    "        \"\"\"\n",
    "        Capture first entry of medium and source. Subsequent entry is not considered as the result \n",
    "        in normalized SQL database \n",
    "            #1. list top medium and source \n",
    "            #2. Clean up records with null values \n",
    "            #3  Exclude all columns except medium and source \n",
    "            #4. Count of distinct utm_medium and utm_source   \n",
    "        \"\"\"\n",
    "        try:\n",
    "            #top_source_medium_df = []          \n",
    "            top_source_medium_sql = \"\"\"\n",
    "            select utm_medium , utm_source , count(*) top_count\n",
    "            from source_df \n",
    "            where \n",
    "            utm_medium is not Null \n",
    "            and utm_source is not  Null \n",
    "            group by utm_medium , utm_source \n",
    "            order by top_count desc \"\"\"\n",
    "\n",
    "            #top_source_medium_df = self.runSQL(top_source_medium_sql)\n",
    "            self.log.logger.info('Start : Distinct cout of UTM SOURCE and MEDIDUM')\n",
    "            drop_null_df = self.source_df.dropna()\n",
    "            top_source_medium_df = drop_null_df.groupby(['utm_medium','utm_source']).size().reset_index(name='counts')\n",
    "            top_source_medium_df = top_source_medium_df.sort_values(['counts'], ascending = False)\n",
    "            #print(top_source_medium_df)\n",
    "            self.log.logger.info('End : Distinct cout of UTM SOURCE & MEDIDUM')\n",
    "            \n",
    "            return top_source_medium_df\n",
    "       \n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing final_dataframe function\")\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    def calculate_metrics(self , in_sliced_df ):\n",
    "        \"\"\"\n",
    "        Distinct users in the set of rec_count records \n",
    "        Minimum time & Maximum Time - Earliest and Last login Time \n",
    "        This is run for every subset of  records in the sliced dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            \n",
    "            min_time = min(in_sliced_df['time'])\n",
    "            max_time = max(in_sliced_df['time'])\n",
    "           # distinct_count_userid = len(in_sliced_df.nunique(axis= 'anonymous_user_id'))\n",
    "            distinct_count_userid = in_sliced_df['anonymous_user_id'].nunique()\n",
    "            #print(min_time)\n",
    "\n",
    "            return min_time, max_time , distinct_count_userid\n",
    "        \n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing calculate_metrics function\")\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "    \n",
    "    def calc_distinctusers_perday(self ):\n",
    "        \"\"\"\n",
    "        Calculate distinct users per day \n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            self.log.logger.info(\"Start :Function: calc_distinctusers_perday\")\n",
    "            #self.log.logger.info(self.source_df.head(1))\n",
    "            self.source_df['time'] = self.source_df['time'].dt.date\n",
    "            grouped_df = self.source_df.groupby('time')\n",
    "            grouped_df = grouped_df.agg({'anonymous_user_id': 'nunique'})\n",
    "            grouped_df = grouped_df.reset_index()\n",
    "            #print(grouped_df)\n",
    "            self.log.logger.info(\"End : calc_distinctusers_perday\")\n",
    "            return grouped_df\n",
    "          \n",
    "        \n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing calc_distinctusers_perday function\")\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "            \n",
    "\n",
    "    def execute_metrics (self , subset_count):\n",
    "        \"\"\"\n",
    "        Execute Calculate metrics on ordered dataframe \n",
    "        \"\"\" \n",
    "        try:\n",
    "            \n",
    "            orderby_final_df= self.source_df.sort_values(by=['time'])\n",
    "            col_names = [\"min_time\" , \"max_time\", \"count_distinct_id\"]\n",
    "            df_metrics_out = pd.DataFrame(columns = col_names)\n",
    "            self.log.logger.info(\"Start : Function execute_metrics\")\n",
    "\n",
    "            for i in range(len(orderby_final_df)):\n",
    "                if ((i+1)%subset_count) == 0 :\n",
    "                    v_end = i \n",
    "                    v_start = i - (subset_count -1)\n",
    "                    sliced_df = orderby_final_df[v_start:v_end]\n",
    "                    min_time, max_time, distinct_count = self.calculate_metrics(sliced_df)\n",
    "                    df_metrics_out = df_metrics_out.append(pd.DataFrame([[min_time, max_time, distinct_count]] , columns = df_metrics_out.columns))\n",
    "            self.log.logger.info(\"End : Function execute_metrics\")           \n",
    "            return  df_metrics_out  \n",
    "        \n",
    "        except:\n",
    "            self.log.logger.error(\"Error executing execute_metrics function\")\n",
    "            self.log.logger.error(sys.exc_info()[0])\n",
    "            \n",
    "        \n",
    "        \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          anonymous_user_id  \\\n",
      "0      b527ad6c-1e79-4ae7-8206-2bf4d127ec25   \n",
      "1      2847753f-df48-4367-98dc-9028f6330532   \n",
      "2      7540e6a6-bd75-4f2b-b774-6309bda47c4f   \n",
      "3      db016ab6-daa3-45fb-84e6-7e6e4e1541ac   \n",
      "4      864557aa-10c5-4efa-a53e-33f7b8f1bd3c   \n",
      "...                                     ...   \n",
      "89944  23c3dac4-513d-4aa7-8646-af11de4d11ff   \n",
      "89945  3b0e66a9-10c8-4c55-bd28-9e6583939104   \n",
      "89946  5eedf75e-d0b4-4b36-b057-945f44edb4ef   \n",
      "89947  9745d1f0-d957-4691-990b-f151e0a34dc1   \n",
      "89948  de76157f-a24e-4e17-b742-650c74acf32b   \n",
      "\n",
      "                                                     url                time  \\\n",
      "0      https://preview.hs-sites.com/_hcms/preview/tem... 2018-04-12 16:50:55   \n",
      "1      https://www.nasa.com/employee-feedback/?utm_me... 2018-04-12 16:16:36   \n",
      "2                                  https://www.nasa.com/ 2018-04-12 16:17:48   \n",
      "3      https://www.nasa.com/products/employee-engagem... 2018-04-12 16:18:33   \n",
      "4                https://www.nasa.com/employee-feedback/ 2018-04-12 16:01:47   \n",
      "...                                                  ...                 ...   \n",
      "89944  https://www.nasa.com/employee-feedback/?utm_me... 2018-04-07 21:11:06   \n",
      "89945  https://www.nasa.com/employee-feedback/?utm_me... 2018-04-07 21:23:27   \n",
      "89946  https://www.nasa.com/employee-feedback/?utm_me... 2018-04-07 21:06:45   \n",
      "89947  https://www.nasa.com/employee-feedback/?utm_me... 2018-04-07 21:35:30   \n",
      "89948  https://www.nasa.com/employee-feedback/?utm_me... 2018-04-07 21:12:28   \n",
      "\n",
      "             browser           os screen_resolution utm_medium utm_source  \\\n",
      "0             Chrome        Linux           800x600       None       None   \n",
      "1             Chrome    Chrome OS          1366x768        cpc     google   \n",
      "2             Chrome     Mac OS X         2560x1440       None       None   \n",
      "3             Chrome     Mac OS X          1440x900       None       None   \n",
      "4             Chrome  Windows 8.1          1366x768       None       None   \n",
      "...              ...          ...               ...        ...        ...   \n",
      "89944  Mobile Safari          iOS          768x1024        cpc     google   \n",
      "89945  Chrome Mobile      Android           360x640        cpc     google   \n",
      "89946  Chrome Mobile      Android           360x640        cpc     google   \n",
      "89947  Chrome Mobile      Android           360x640        cpc     google   \n",
      "89948  Mobile Safari          iOS           375x667        cpc     google   \n",
      "\n",
      "                                 path  \n",
      "0       /_hcms/preview/template/multi  \n",
      "1                 /employee-feedback/  \n",
      "2                                   /  \n",
      "3      /products/employee-engagement/  \n",
      "4                 /employee-feedback/  \n",
      "...                               ...  \n",
      "89944             /employee-feedback/  \n",
      "89945             /employee-feedback/  \n",
      "89946             /employee-feedback/  \n",
      "89947             /employee-feedback/  \n",
      "89948             /employee-feedback/  \n",
      "\n",
      "[89949 rows x 9 columns]\n",
      "      utm_medium                utm_source  counts\n",
      "3            cpc                    google   52386\n",
      "6          email                  hs_email     981\n",
      "12    paidsocial                  linkedin     363\n",
      "11    paidsocial                  facebook      93\n",
      "5          email             hs_automation      62\n",
      "7          email  hs_email---hs_automation      20\n",
      "14        social                   twitter      18\n",
      "9   paid_listing                  capterra      14\n",
      "15        social               twitter.com       6\n",
      "0     Display Ad                Office Kit       4\n",
      "10  paid_listing             capterra---ga       3\n",
      "4            cpc                       ppc       2\n",
      "1       Facebook                    Social       1\n",
      "2            cpc                    carbon       1\n",
      "8          email                  sendgrid       1\n",
      "13      referral                    wework       1\n",
      "             min_time            max_time count_distinct_id\n",
      "0 2018-04-01 00:03:09 2018-04-03 14:42:26              6101\n",
      "0 2018-04-03 14:43:25 2018-04-04 23:55:30              6387\n",
      "0 2018-04-04 23:55:40 2018-04-06 14:41:55              6615\n",
      "0 2018-04-06 14:42:06 2018-04-08 03:25:48              7310\n",
      "0 2018-04-08 03:26:28 2018-04-09 19:37:50              6964\n",
      "0 2018-04-09 19:38:02 2018-04-10 21:03:34              6746\n",
      "0 2018-04-10 21:03:51 2018-04-11 23:45:18              6532\n",
      "0 2018-04-11 23:45:20 2018-04-13 03:33:40              6854\n",
      "          time  anonymous_user_id\n",
      "0   2018-04-01               1161\n",
      "1   2018-04-02               2962\n",
      "2   2018-04-03               3538\n",
      "3   2018-04-04               5087\n",
      "4   2018-04-05               4146\n",
      "5   2018-04-06               4886\n",
      "6   2018-04-07               4160\n",
      "7   2018-04-08               4198\n",
      "8   2018-04-09               5343\n",
      "9   2018-04-10               6146\n",
      "10  2018-04-11               5679\n",
      "11  2018-04-12               5566\n",
      "12  2018-04-13               5279\n",
      "13  2018-04-14               3103\n"
     ]
    }
   ],
   "source": [
    "v_input_file_name = \"data.csv\"\n",
    "source_df = pd.read_csv(v_input_file_name)\n",
    "t = Transformation(source_df)\n",
    "t.set_time_readable()\n",
    "t.set_medium_source_path()\n",
    "print(source_df)\n",
    "top_source_medium_df= t.final_dataframe()\n",
    "print(top_source_medium_df)\n",
    "\n",
    "df_metrics_out = t.execute_metrics( 10000)  \n",
    "print(df_metrics_out) \n",
    "x = t.calc_distinctusers_perday()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimic artifical window function \n",
    "# Generate output in the form of stream based on number of records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['anonymous_user_id', 'time']\n",
    "jsonl_df = pd.DataFrame(columns = cols )\n",
    "#print(type(jsonl_df))\n",
    "\n",
    "def calc_metrics_window(sliced_df , row_count):\n",
    "    \"\"\"\n",
    "    Create a artificial window in terms of record count \n",
    "    Source data = consider streaming scenario \n",
    "    Outout in form of json file with individual JSONL records \n",
    "\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "    select count(distinct(anonymous_user_id)) as count_distinct_id , \n",
    "    min(time) as min_time, \n",
    "    max(time) as max_time \n",
    "    from sliced_df  \n",
    "    \"\"\"\n",
    "    out = runSQL(sql)\n",
    "    return out['count_distinct_id'][0], out ['min_time'][0] , out['max_time'][0]\n",
    "    \n",
    "    \n",
    "\n",
    "with open('data.json1', 'rb') as input_file:\n",
    "    row_count = 0 \n",
    "    for line in json_lines.reader(input_file):\n",
    "        new_row = {'anonymous_user_id' : line['anonymous_user_id'], 'time': pd.to_datetime((line['time']),unit='ns')}\n",
    "        jsonl_df = jsonl_df.append(new_row, ignore_index=True)\n",
    "        row_count = row_count + 1 \n",
    "        \n",
    "        if (row_count%1000 == 0):\n",
    "            #break \n",
    "            v_end = row_count\n",
    "            v_start = row_count - 1000\n",
    "            sliced_df = jsonl_df[v_start:v_end]\n",
    "            count_distinct_id, min_time , max_time = calc_metrics_window(sliced_df, row_count)\n",
    "            final_metrics = {'count_distinct_id':str(count_distinct_id) ,'min_time':str(min_time), 'max_time':str(max_time) }\n",
    "            #print (count_distinct_id,min_time,max_time  )\n",
    "            metrics_json = json.dumps(final_metrics)\n",
    "            out_file_name = \"out_metrics\"+\".json\"\n",
    "            outfile = open(out_file_name , \"a\")\n",
    "            outfile.write(metrics_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
